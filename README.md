# Description

- This is CPU-only queue-based inference system for running local LLMs under constrained resources.
- I built this out of curiosity to learn Queue + LLM inference on CPU.
- I made this `README.md` as readable as possible i hope this helps.

# Architecture
